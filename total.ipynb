{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "PaddlePaddle 1.6.0 (Python 3.5)",
      "language": "python",
      "name": "py35-paddle1.2.0"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "usable.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzKAQJ1H4YOi",
        "colab_type": "code",
        "outputId": "703f2561-9aba-492d-9a84-00fee8bacaf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOAVCNJZ4Blo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import sys\r\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER-XtHJF4Blt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections.abc import Sequence\r\n",
        "import random\r\n",
        "import skimage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr59lIBP4Bly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/train_val.csv')\n",
        "result=pd.read_csv('/content/drive/My Drive/sampleSubmission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpt662Fk4Bl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\r\n",
        "from itertools import repeat\r\n",
        "import numpy as np\r\n",
        "import scipy\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from skimage.measure import find_contours\r\n",
        "def plot_voxel(arr, aux=None):\r\n",
        "    if aux is not None:\r\n",
        "        assert arr.shape == aux.shape\r\n",
        "    length = arr.shape[0]\r\n",
        "    _, axes = plt.subplots(length, 1, figsize=(4, 4 * length))\r\n",
        "    for i, ax in enumerate(axes):\r\n",
        "        ax.set_title(\"@%s\" % i)\r\n",
        "        ax.imshow(arr[i], cmap=plt.cm.gray)\r\n",
        "        if aux is not None:\r\n",
        "            ax.imshow(aux[i], alpha=0.3)\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "\r\n",
        "def plot_voxel_save(path, arr, aux=None):\r\n",
        "    if aux is not None:\r\n",
        "        assert arr.shape == aux.shape\r\n",
        "    length = arr.shape[0]\r\n",
        "    for i in range(length):\r\n",
        "        plt.clf()\r\n",
        "        plt.title(\"@%s\" % i)\r\n",
        "        plt.imshow(arr[i], cmap=plt.cm.gray)\r\n",
        "        if aux is not None:\r\n",
        "            plt.imshow(aux[i], alpha=0.2)\r\n",
        "        plt.savefig(path + \"%s.png\" % i)\r\n",
        "\r\n",
        "\r\n",
        "def plot_voxel_enhance(arr, arr_mask=None, figsize=10, alpha=0.1):  # zyx\r\n",
        "    '''borrow from yuxiang.'''\r\n",
        "    plt.figure(figsize=(figsize, figsize))\r\n",
        "    rows = cols = int(round(np.sqrt(arr.shape[0])))\r\n",
        "    img_height = arr.shape[1]\r\n",
        "    img_width = arr.shape[2]\r\n",
        "    assert img_width == img_height\r\n",
        "    res_img = np.zeros((rows * img_height, cols * img_width), dtype=np.uint8)\r\n",
        "    if arr_mask is not None:\r\n",
        "        res_mask_img = np.zeros(\r\n",
        "            (rows * img_height, cols * img_width), dtype=np.uint8)\r\n",
        "    for row in range(rows):\r\n",
        "        for col in range(cols):\r\n",
        "            if (row * cols + col) >= arr.shape[0]:\r\n",
        "                continue\r\n",
        "            target_y = row * img_height\r\n",
        "            target_x = col * img_width\r\n",
        "            res_img[target_y:target_y + img_height,\r\n",
        "            target_x:target_x + img_width] = arr[row * cols + col]\r\n",
        "            if arr_mask is not None:\r\n",
        "                res_mask_img[target_y:target_y + img_height,\r\n",
        "                target_x:target_x + img_width] = arr_mask[row * cols + col]\r\n",
        "    plt.imshow(res_img, plt.cm.gray)\r\n",
        "    if arr_mask is not None:\r\n",
        "        plt.imshow(res_mask_img, alpha=alpha)\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "\r\n",
        "def find_edges(mask, level=0.5):\r\n",
        "    edges = find_contours(mask, level)[0]\r\n",
        "    ys = edges[:, 0]\r\n",
        "    xs = edges[:, 1]\r\n",
        "    return xs, ys\r\n",
        "\r\n",
        "\r\n",
        "def plot_contours(arr, aux, level=0.5, ax=None, **kwargs):\r\n",
        "    if ax is None:\r\n",
        "        _, ax = plt.subplots(1, 1, **kwargs)\r\n",
        "    ax.imshow(arr, cmap=plt.cm.gray)\r\n",
        "    xs, ys = find_edges(aux, level)\r\n",
        "    ax.plot(xs, ys)\r\n",
        "\r\n",
        "\r\n",
        "def crop_at_zyx_with_dhw(voxel, zyx, dhw, fill_with):\r\n",
        "    '''Crop and pad on the fly.'''\r\n",
        "    shape = voxel.shape\r\n",
        "    # z, y, x = zyx\r\n",
        "    # d, h, w = dhw\r\n",
        "    crop_pos = []\r\n",
        "    padding = [[0, 0], [0, 0], [0, 0]]\r\n",
        "    for i, (center, length) in enumerate(zip(zyx, dhw)):\r\n",
        "        assert length % 2 == 0\r\n",
        "        # assert center < shape[i] # it's not necessary for \"moved center\"\r\n",
        "        low = round(center) - length // 2\r\n",
        "        high = round(center) + length // 2\r\n",
        "        if low < 0:\r\n",
        "            padding[i][0] = int(0 - low)\r\n",
        "            low = 0\r\n",
        "        if high > shape[i]:\r\n",
        "            padding[i][1] = int(high - shape[i])\r\n",
        "            high = shape[i]\r\n",
        "        crop_pos.append([int(low), int(high)])\r\n",
        "    cropped = voxel[crop_pos[0][0]:crop_pos[0][1], crop_pos[1]\r\n",
        "                                                   [0]:crop_pos[1][1], crop_pos[2][0]:crop_pos[2][1]]\r\n",
        "    if np.sum(padding) > 0:\r\n",
        "        cropped = np.lib.pad(cropped, padding, 'constant',\r\n",
        "                             constant_values=fill_with)\r\n",
        "    return cropped\r\n",
        "\r\n",
        "\r\n",
        "def window_clip(v, window_low=-1024, window_high=400, dtype=np.uint8):\r\n",
        "    '''Use lung windown to map CT voxel to grey.'''\r\n",
        "    # assert v.min() <= window_low\r\n",
        "    return np.round(np.clip((v - window_low) / (window_high - window_low) * 255., 0, 255)).astype(dtype)\r\n",
        "\r\n",
        "\r\n",
        "def resize(voxel, spacing, new_spacing=[1., 1., 1.]):\r\n",
        "    '''Resize `voxel` from `spacing` to `new_spacing`.'''\r\n",
        "    resize_factor = []\r\n",
        "    for sp, nsp in zip(spacing, new_spacing):\r\n",
        "        resize_factor.append(float(sp) / nsp)\r\n",
        "    resized = scipy.ndimage.interpolation.zoom(voxel, resize_factor, mode='nearest')\r\n",
        "    for i, (sp, shape, rshape) in enumerate(zip(spacing, voxel.shape, resized.shape)):\r\n",
        "        new_spacing[i] = float(sp) * shape / rshape\r\n",
        "    return resized, new_spacing\r\n",
        "\r\n",
        "\r\n",
        "def rotation(array, angle):\r\n",
        "    '''using Euler angles method.\r\n",
        "    @author: renchao\r\n",
        "    @params:\r\n",
        "        angle: 0: no rotation, 1: rotate 90 deg, 2: rotate 180 deg, 3: rotate 270 deg\r\n",
        "    '''\r\n",
        "    #\r\n",
        "    X = np.rot90(array, angle[0], axes=(0, 1))  # rotate in X-axis\r\n",
        "    Y = np.rot90(X, angle[1], axes=(0, 2))  # rotate in Y'-axis\r\n",
        "    Z = np.rot90(Y, angle[2], axes=(1, 2))  # rotate in Z\"-axis\r\n",
        "    return Z\r\n",
        "\r\n",
        "\r\n",
        "def reflection(array, axis):\r\n",
        "    '''\r\n",
        "    @author: renchao\r\n",
        "    @params:\r\n",
        "        axis: -1: no flip, 0: Z-axis, 1: Y-axis, 2: X-axis\r\n",
        "    '''\r\n",
        "    if axis != -1:\r\n",
        "        ref = np.flip(array, axis)\r\n",
        "    else:\r\n",
        "        ref = np.copy(array)\r\n",
        "    return ref\r\n",
        "\r\n",
        "\r\n",
        "def crop(array, zyx, dhw):\r\n",
        "    z, y, x = zyx\r\n",
        "    d, h, w = dhw\r\n",
        "    cropped = array[z - d // 2:z + d // 2,\r\n",
        "              y - h // 2:y + h // 2,\r\n",
        "              x - w // 2:x + w // 2]\r\n",
        "    return cropped\r\n",
        "\r\n",
        "\r\n",
        "def random_center(shape, move):\r\n",
        "    offset = np.random.randint(-move, move + 1, size=3)\r\n",
        "    zyx = np.array(shape) // 2 + offset\r\n",
        "    return zyx\r\n",
        "\r\n",
        "\r\n",
        "def get_uniform_assign(length, subset):\r\n",
        "    assert subset > 0\r\n",
        "    per_length, remain = divmod(length, subset)\r\n",
        "    total_set = np.random.permutation(list(range(subset)) * per_length)\r\n",
        "    remain_set = np.random.permutation(list(range(subset)))[:remain]\r\n",
        "    return list(total_set) + list(remain_set)\r\n",
        "\r\n",
        "\r\n",
        "def split_validation(df, subset, by):\r\n",
        "    df = df.copy()\r\n",
        "    for sset in df[by].unique():\r\n",
        "        length = (df[by] == sset).sum()\r\n",
        "        df.loc[df[by] == sset, 'subset'] = get_uniform_assign(length, subset)\r\n",
        "    df['subset'] = df['subset'].astype(int)\r\n",
        "    return df\r\n",
        "\r\n",
        "\r\n",
        "def _ntuple(n):\r\n",
        "    def parse(x):\r\n",
        "        if isinstance(x, collections.Iterable):\r\n",
        "            return x\r\n",
        "        return tuple(repeat(x, n))\r\n",
        "\r\n",
        "    return parse\r\n",
        "_single = _ntuple(1)\r\n",
        "_pair = _ntuple(2)\r\n",
        "_triple = _ntuple(3)\r\n",
        "_quadruple = _ntuple(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DIF5uth4Bl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transform:\r\n",
        "\r\n",
        "    def __init__(self, size, move):\r\n",
        "        self.size = _triple(size)\r\n",
        "        self.move = move\r\n",
        "\r\n",
        "    def __call__(self, arr, aux=None):\r\n",
        "        shape = arr.shape\r\n",
        "        if self.move is None:\r\n",
        "            # center = random_center(shape, self.move)\r\n",
        "            center = np.array(shape) // 2\r\n",
        "            arr_ret = crop(arr, center, self.size)\r\n",
        "            angle = np.random.randint(4, size=3)\r\n",
        "            arr_ret = rotation(arr_ret, angle=angle)\r\n",
        "            axis = np.random.randint(4) - 1\r\n",
        "            arr_ret = reflection(arr_ret, axis=axis)\r\n",
        "            arr_ret = np.expand_dims(arr_ret, axis=-1)\r\n",
        "            if aux is not None:\r\n",
        "                aux_ret = crop(aux, center, self.size)\r\n",
        "                aux_ret = rotation(aux_ret, angle=angle)\r\n",
        "                aux_ret = reflection(aux_ret, axis=axis)\r\n",
        "                aux_ret = np.expand_dims(aux_ret, axis=-1)\r\n",
        "                return arr_ret, aux_ret\r\n",
        "            return arr_ret\r\n",
        "        else:\r\n",
        "            center = np.array(shape) // 2\r\n",
        "            arr_ret = crop(arr, center, self.size)\r\n",
        "            arr_ret = np.expand_dims(arr_ret, axis=-1)\r\n",
        "            if aux is not None:\r\n",
        "                aux_ret = crop(aux, center, self.size)\r\n",
        "                aux_ret = np.expand_dims(aux_ret, axis=-1)\r\n",
        "                return arr_ret, aux_ret\r\n",
        "            return arr_ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROj8nSgz4BmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClfDataset(Sequence):\r\n",
        "    def __init__(self, crop_size=32, move=None):\r\n",
        "        self.transform = Transform(crop_size,move)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        name = df.loc[item, 'name']\r\n",
        "        with np.load(os.path.join('/content/drive/My Drive/train/train_val', '%s.npz' % name)) as npz:\r\n",
        "            voxel, seg = self.transform(npz['voxel'], npz['seg'])\r\n",
        "            # voxel = voxel*seg\r\n",
        "        label = df.loc[item, 'lable']\r\n",
        "        index_generator = shuffle_iterator(range(365))\r\n",
        "        item_new = next(index_generator)\r\n",
        "        name_new = df.loc[item_new, 'name']\r\n",
        "        with np.load(os.path.join('/content/drive/My Drive/train/train_val', '%s.npz' % name_new)) as npz:\r\n",
        "            voxel_new, seg_new = self.transform(npz['voxel'], npz['seg'])\r\n",
        "        label_new = df.loc[item_new, 'lable']\r\n",
        "        a=random.randint(1,9)/10\r\n",
        "        voxel = np.around(a*voxel+(1-a)*voxel_new,0)\r\n",
        "        label = np.around(a*label+(1-a)*label_new,0)\r\n",
        "        # if label>0.5:\r\n",
        "        #     label=1\r\n",
        "        # else:\r\n",
        "        #     label=0\r\n",
        "        seg = np.around(a*seg+(1-a)*seg_new,0)\r\n",
        "        return voxel,  (label, seg)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return df.__len__()\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def _collate_fn(data):\r\n",
        "        xs = []\r\n",
        "        ys = []\r\n",
        "        segs = []\r\n",
        "        for x, y in data:\r\n",
        "            xs.append(x)\r\n",
        "            ys.append(y[0])\r\n",
        "            segs.append(y[1])\r\n",
        "        # return np.array([xs]).transpose(1, 2, 3, 4, 0), np.array(ys)\r\n",
        "        return np.array(xs), {\"clf\": np.array(ys), \"seg\": np.array(segs)}\r\n",
        "\r\n",
        "class ClfDataset_val(Sequence):\r\n",
        "    def __init__(self, crop_size=32, move=None):\r\n",
        "        self.transform = Transform(crop_size,move)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        name = df.loc[item, 'name']\r\n",
        "        with np.load(os.path.join('/content/drive/My Drive/train/train_val', '%s.npz' % name)) as npz:\r\n",
        "            voxel, seg = self.transform(npz['voxel'], npz['seg'])\r\n",
        "            # voxel = voxel*seg\r\n",
        "        label = df.loc[item, 'lable']\r\n",
        "        return voxel,  (label, seg)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return df.__len__()\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def _collate_fn(data):\r\n",
        "        xs = []\r\n",
        "        ys = []\r\n",
        "        segs = []\r\n",
        "        for x, y in data:\r\n",
        "            xs.append(x)\r\n",
        "            ys.append(y[0])\r\n",
        "            segs.append(y[1])\r\n",
        "        # return np.array([xs]).transpose(1, 2, 3, 4, 0), np.array(ys)\r\n",
        "        return np.array(xs), {\"clf\": np.array(ys), \"seg\": np.array(segs)}\r\n",
        "\r\n",
        "class ClfDataset_test(Sequence):\r\n",
        "    def __init__(self, crop_size=32, move=None):\r\n",
        "        self.transform = Transform(crop_size,move)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        name = result.loc[item, 'name']\r\n",
        "        with np.load(os.path.join('/content/drive/My Drive/test', '%s.npz' % name)) as npz:\r\n",
        "            voxel, seg = self.transform(npz['voxel'], npz['seg'])\r\n",
        "            # voxel = voxel*seg\r\n",
        "        # label = df.loc[item, 'lable']\r\n",
        "        return voxel\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        \r\n",
        "        \r\n",
        "        return df.__len__()\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def _collate_fn(data):\r\n",
        "        xs = []\r\n",
        "        for x in data:\r\n",
        "            xs.append(x)\r\n",
        "        # return np.array([xs]).transpose(1, 2, 3, 4, 0), np.array(ys)\r\n",
        "        return np.array(xs)\r\n",
        "        \r\n",
        "        \r\n",
        "def shuffle_iterator(iterator):\r\n",
        "    # iterator should have limited size\r\n",
        "    index = list(iterator)\r\n",
        "    total_size = len(index)\r\n",
        "    i = 0\r\n",
        "    random.shuffle(index)\r\n",
        "    while True:\r\n",
        "        yield index[i]\r\n",
        "        i += 1\r\n",
        "        if i >= total_size:\r\n",
        "            i = 0\r\n",
        "            random.shuffle(index)\r\n",
        "\r\n",
        "def get_loader_train(dataset, batch_size):\r\n",
        "    total_size = 365\r\n",
        "    print('Size', total_size)\r\n",
        "    index_generator = shuffle_iterator(range(total_size))\r\n",
        "    while True:\r\n",
        "        data = []\r\n",
        "        for _ in range(batch_size):\r\n",
        "            idx = next(index_generator)\r\n",
        "            # idx2 = next(index_generator)\r\n",
        "            data.append(dataset[idx])\r\n",
        "        yield dataset._collate_fn(data)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def get_loader_val(dataset, batch_size):\r\n",
        "    total_size = 100\r\n",
        "    print('Size', total_size)\r\n",
        "    index_generator = shuffle_iterator(range(total_size))\r\n",
        "    while True:\r\n",
        "        data = []\r\n",
        "        for _ in range(batch_size):\r\n",
        "            idx = next(index_generator)\r\n",
        "            data.append(dataset[idx+365])\r\n",
        "        yield dataset._collate_fn(data)\r\n",
        "        \r\n",
        "def get_loader_test(dataset, batch_size):\r\n",
        "    total_size = 117\r\n",
        "    print('Size', total_size)\r\n",
        "    # index_generator = shuffle_iterator(range(total_size))\r\n",
        "    while True:\r\n",
        "        data = []\r\n",
        "        for i in range(total_size):\r\n",
        "            # idx = next(index_generator)\r\n",
        "            idx=i\r\n",
        "            data.append(dataset[idx])\r\n",
        "        yield dataset._collate_fn(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSMh42l84BmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = ClfDataset(crop_size=32)\r\n",
        "dataset_val = ClfDataset_val(crop_size=32)\r\n",
        "test_dataset = ClfDataset_test(crop_size=32)\r\n",
        "train_loader = get_loader_train(dataset, batch_size=50)\r\n",
        "val_loader = get_loader_val(dataset_val, batch_size=50)\r\n",
        "test_loader = get_loader_test(test_dataset, batch_size=117)\r\n",
        "\r\n",
        "learning_rate=1.e-4\r\n",
        "segmentation_task_ratio=0.1\r\n",
        "weight_decay=0.\r\n",
        "save_folder='test'\r\n",
        "epochs=30\r\n",
        "PARAMS = {\r\n",
        "    'activation': lambda: Activation('relu'),  # the activation functions\r\n",
        "    'bn_scale': True,  # whether to use the scale function in BN\r\n",
        "    'weight_decay': 0.,  # l2 weight decay\r\n",
        "    'kernel_initializer': 'he_uniform',  # initialization\r\n",
        "    'first_scale': lambda x: x / 128. - 1.,  # the first pre-processing function\r\n",
        "    'dhw': [32,32,32],  # the input shape\r\n",
        "    'k': 16,  # the `growth rate` in DenseNet\r\n",
        "    'bottleneck': 4,  # the `bottleneck` in DenseNet\r\n",
        "    'compression': 2,  # the `compression` in DenseNet\r\n",
        "    'first_layer': 32,  # the channel of the first layer\r\n",
        "    'down_structure': [4, 4, 4],  # the down-sample structure\r\n",
        "    'output_size': 1,  # the output number of the classification head\r\n",
        "    'dropout_rate': None  # whether to use dropout, and how much to use\r\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_vPmpw14BmT",
        "colab_type": "code",
        "outputId": "84bb37b2-7f65-4ae3-e7a6-fcb2259af698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, EarlyStopping, ReduceLROnPlateau\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.layers import (Conv3D, BatchNormalization, AveragePooling3D, concatenate, Lambda, SpatialDropout3D,\r\n",
        "                          Activation, Input, GlobalAvgPool3D, Dense, Conv3DTranspose, add)\r\n",
        "from keras.regularizers import l2 as l2_penalty\r\n",
        "from keras.models import Model\r\n",
        "import keras.backend as K"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYbHO2Zm4Bmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _conv_block(x, filters):\r\n",
        "    bn_scale = PARAMS['bn_scale']\r\n",
        "    activation = PARAMS['activation']\r\n",
        "    kernel_initializer = PARAMS['kernel_initializer']\r\n",
        "    weight_decay = PARAMS['weight_decay']\r\n",
        "    bottleneck = PARAMS['bottleneck']\r\n",
        "    dropout_rate = PARAMS['dropout_rate']\r\n",
        "\r\n",
        "    x = BatchNormalization(scale=bn_scale, axis=-1)(x)\r\n",
        "    x = activation()(x)\r\n",
        "    x = Conv3D(filters * bottleneck, kernel_size=(1, 1, 1), padding='same', use_bias=False,\r\n",
        "               kernel_initializer=kernel_initializer,\r\n",
        "               kernel_regularizer=l2_penalty(weight_decay))(x)\r\n",
        "    if dropout_rate is not None:\r\n",
        "        x = SpatialDropout3D(dropout_rate)(x)\r\n",
        "    x = BatchNormalization(scale=bn_scale, axis=-1)(x)\r\n",
        "    x = activation()(x)\r\n",
        "    x = Conv3D(filters, kernel_size=(3, 3, 3), padding='same', use_bias=True,\r\n",
        "               kernel_initializer=kernel_initializer,\r\n",
        "               kernel_regularizer=l2_penalty(weight_decay))(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def _dense_block(x, n):\r\n",
        "    k = PARAMS['k']\r\n",
        "\r\n",
        "    for _ in range(n):\r\n",
        "        conv = _conv_block(x, k)\r\n",
        "        x = concatenate([conv, x], axis=-1)\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def _transmit_block(x, is_last):\r\n",
        "    bn_scale = PARAMS['bn_scale']\r\n",
        "    activation = PARAMS['activation']\r\n",
        "    kernel_initializer = PARAMS['kernel_initializer']\r\n",
        "    weight_decay = PARAMS['weight_decay']\r\n",
        "    compression = PARAMS['compression']\r\n",
        "\r\n",
        "    x = BatchNormalization(scale=bn_scale, axis=-1)(x)\r\n",
        "    x = activation()(x)\r\n",
        "    if is_last:\r\n",
        "        x = GlobalAvgPool3D()(x)\r\n",
        "    else:\r\n",
        "        *_, f = x.get_shape().as_list()\r\n",
        "        x = Conv3D(f // compression, kernel_size=(1, 1, 1), padding='same', use_bias=True,\r\n",
        "                   kernel_initializer=kernel_initializer,\r\n",
        "                   kernel_regularizer=l2_penalty(weight_decay))(x)\r\n",
        "        x = AveragePooling3D((2, 2, 2), padding='valid')(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def get_model(weights=None, verbose=True, **kwargs):\r\n",
        "    for k, v in kwargs.items():\r\n",
        "        assert k in PARAMS\r\n",
        "        PARAMS[k] = v\r\n",
        "    if verbose:\r\n",
        "        print(\"Model hyper-parameters:\", PARAMS)\r\n",
        "\r\n",
        "    dhw = PARAMS['dhw']\r\n",
        "    first_scale = PARAMS['first_scale']\r\n",
        "    first_layer = PARAMS['first_layer']\r\n",
        "    kernel_initializer = PARAMS['kernel_initializer']\r\n",
        "    weight_decay = PARAMS['weight_decay']\r\n",
        "    down_structure = PARAMS['down_structure']\r\n",
        "    output_size = PARAMS['output_size']\r\n",
        "\r\n",
        "    shape = dhw + [1]\r\n",
        "\r\n",
        "    inputs = Input(shape=shape)\r\n",
        "\r\n",
        "    if first_scale is not None:\r\n",
        "        scaled = Lambda(first_scale)(inputs)\r\n",
        "    else:\r\n",
        "        scaled = inputs\r\n",
        "    conv = Conv3D(first_layer, kernel_size=(3, 3, 3), padding='same', use_bias=True,\r\n",
        "                  kernel_initializer=kernel_initializer,\r\n",
        "                  kernel_regularizer=l2_penalty(weight_decay))(scaled)\r\n",
        "\r\n",
        "    downsample_times = len(down_structure)\r\n",
        "    top_down = []\r\n",
        "    for l, n in enumerate(down_structure):\r\n",
        "        db = _dense_block(conv, n)\r\n",
        "        top_down.append(db)\r\n",
        "        conv = _transmit_block(db, l == downsample_times - 1)\r\n",
        "\r\n",
        "    feat = top_down[-1]\r\n",
        "    for top_feat in reversed(top_down[:-1]):\r\n",
        "        *_, f = top_feat.get_shape().as_list()\r\n",
        "        deconv = Conv3DTranspose(filters=f, kernel_size=2, strides=2, use_bias=True,\r\n",
        "                                 kernel_initializer=kernel_initializer,\r\n",
        "                                 kernel_regularizer=l2_penalty(weight_decay))(feat)\r\n",
        "        feat = add([top_feat, deconv])\r\n",
        "    seg_head = Conv3D(1, kernel_size=(1, 1, 1), padding='same',\r\n",
        "                      activation='sigmoid', use_bias=True,\r\n",
        "                      kernel_initializer=kernel_initializer,\r\n",
        "                      kernel_regularizer=l2_penalty(weight_decay),\r\n",
        "                      name='seg')(feat)\r\n",
        "\r\n",
        "    if output_size == 1:\r\n",
        "        last_activation = 'sigmoid'\r\n",
        "    else:\r\n",
        "        last_activation = 'softmax'\r\n",
        "\r\n",
        "    clf_head = Dense(output_size, activation=last_activation,\r\n",
        "                     kernel_regularizer=l2_penalty(weight_decay),\r\n",
        "                     kernel_initializer=kernel_initializer,\r\n",
        "                     name='clf')(conv)\r\n",
        "\r\n",
        "    model = Model(inputs, [clf_head, seg_head])\r\n",
        "    if verbose:\r\n",
        "        model.summary()\r\n",
        "\r\n",
        "    if weights is not None:\r\n",
        "        model.load_weights(weights)\r\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDFNXnJC4Bml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DiceLoss:\r\n",
        "    def __init__(self, beta=1., smooth=1.):\r\n",
        "        self.__name__ = 'dice_loss_' + str(int(beta * 100))\r\n",
        "        self.beta = beta  # the more beta, the more recall\r\n",
        "        self.smooth = smooth\r\n",
        "\r\n",
        "    def __call__(self, y_true, y_pred):\r\n",
        "        bb = self.beta * self.beta\r\n",
        "        y_true_f = K.batch_flatten(y_true)\r\n",
        "        y_pred_f = K.batch_flatten(y_pred)\r\n",
        "        intersection = K.sum(y_true_f * y_pred_f, axis=-1)\r\n",
        "        weighted_union = bb * K.sum(y_true_f, axis=-1) + \\\r\n",
        "                         K.sum(y_pred_f, axis=-1)\r\n",
        "        score = -((1 + bb) * intersection + self.smooth) / \\\r\n",
        "                (weighted_union + self.smooth)\r\n",
        "        return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjCCUBz04Bmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = get_model()\r\n",
        "model.compile(optimizer=Adam(lr=1.e-4), loss={\"clf\": 'binary_crossentropy', \"seg\": DiceLoss()},\r\n",
        "              metrics={'clf': 'accuracy', 'seg': 'accuracy'}, loss_weights={\"clf\": 1., \"seg\": -0.1})\r\n",
        "checkpointer = ModelCheckpoint(filepath='/content/drive/My Drive/temp/%s/weights.{epoch:02d}.h5py' % save_folder, verbose=1,\r\n",
        "                              period=1, save_weights_only=True)\r\n",
        "best_keeper = ModelCheckpoint(filepath='/content/drive/My Drive/temp/%s/best.h5py' % save_folder, verbose=1, save_weights_only=True,\r\n",
        "                              monitor='val_clf_accuracy', save_best_only=True, period=1, mode='max')\r\n",
        "csv_logger = CSVLogger('/content/drive/My Drive/temp/%s/training.csv' % save_folder)\r\n",
        "# early_stopping = EarlyStopping(monitor='val_clf_accuracy', min_delta=0, mode='max',\r\n",
        "#                                 patience=30, verbose=1)\r\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.334, patience=10,\r\n",
        "                              verbose=1, mode='min', epsilon=1.e-5, cooldown=2, min_lr=0)\r\n",
        "model.fit_generator(generator=train_loader, steps_per_epoch=30, max_queue_size=500, workers=1,\r\n",
        "                    validation_data=val_loader, epochs=epochs, validation_steps=15,\r\n",
        "                      callbacks=[checkpointer, best_keeper, lr_reducer, csv_logger])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "KucZ214b4Bmx",
        "colab_type": "text"
      },
      "source": [
        "test the model, save the model to \"result.CSV\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjOlaGoT4Bmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = get_model('/content/drive/My Drive/bestweight_xbt_2.h5')\n",
        "model1 = get_model('/content/drive/My Drive/temp/test/weights.19.h5py')#weights in model2 choose NO.17-29\n",
        "model2 = get_model('/content/drive/My Drive/temp/test/weights.30.h5py')#weights in model3 choose NO.29\n",
        "test_data=next(test_loader)\n",
        "print(test_data.shape)\n",
        "b=model.predict(test_data)\n",
        "b1=model1.predict(test_data)\n",
        "b2=model2.predict(test_data)\n",
        "result['Score']=(b1[0]+b2[0])/2\n",
        "save=pd.DataFrame(data=result)\n",
        "save.to_csv('/content/drive/My Drive/temp/result.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw-VzLpR4Bm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}